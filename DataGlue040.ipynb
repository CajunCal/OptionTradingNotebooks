{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataGlue v040\n",
    "Moved to Jupyter Notebook 5/11/20\n",
    "\n",
    "1. Create stock features - SMAs, velocities, oscillators - MACD, RSI, MFI, etc, Days to ER\n",
    "2. Merge / join with Option trade records\n",
    "3. Define label. \n",
    "4. Scaling. Chexk for outliers. Run standardScaler.\n",
    "5. Feature Selection / Reduction - Univariate, Correlation Heat Map, RFE, PCA. Shoose final features for first run.\n",
    "6. Test many Learners against single trades. Pick best few.\n",
    "7. Measure Learners against portfolio metrics. \n",
    "8. Find trade profit % that optimizes portfolio metrics.\n",
    "9. Rerun Learners with that profit% as label.\n",
    "\n",
    "10. Define process to use and test BTC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# BDay is business day, not birthday...\n",
    "from pandas.tseries.offsets import BDay\n",
    "from pandas.tseries.offsets import CustomBusinessDay as CBDay\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Libraries for Visualizers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML Libraries from SKLearn.\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.preprocessing  import StandardScaler\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble       import RandomForestClassifier\n",
    "from sklearn.datasets       import make_classification\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "# FLOW CONTROL AND CONFIGURATION - Flow Control maybe not needed in Notebook?\n",
    "\n",
    "TEST_TYPES        = ('golden test', 'code locator', 'code run times', 'none')  #These drive the test configuration for the run.\n",
    "test_config       = ('none')\n",
    "\n",
    "FEATURE_TYPES     = ('price calc', 'velocity calc')  #price calc automatically does ER calc\n",
    "feature_select    = ('price calc', 'velocity calc')\n",
    "\n",
    "correlate = False  # Run corrrelation section\n",
    "visualize = False   # Run charts for each feature comparing data grouped by label.\n",
    "\n",
    "# CONSTANTS\n",
    "\n",
    "NYSE_HOLIDAYS = ('2017-01-02 00:00:00', '2017-01-16 00:00:00', '2017-02-20 00:00:00', '2017-04-14 00:00:00', \\\n",
    "                 '2017-05-29 00:00:00', '2017-07-04 00:00:00', '2017-09-04 00:00:00', '2017-11-23 00:00:00', '2017-12-25 00:00:00', \\\n",
    "                 '2018-01-01 00:00:00', '2018-01-15 00:00:00', '2018-02-19 00:00:00', '2018-03-30 00:00:00', \\\n",
    "                 '2018-05-28 00:00:00', '2018-07-04 00:00:00', '2018-09-03 00:00:00', '2018-11-22 00:00:00', '2018-12-25')\n",
    "# add NYSE holidays for 2019, 2020 to complete the set.\n",
    "\n",
    "\n",
    "BUCKET_COUNT      = 25  #used for bucket normalizers\n",
    "\n",
    "LOCATIONS = ('initialize', 'price feature calcs', 'join', 'delete xtn cols', 'feature buckets')\n",
    "             #LOCATIONS is the set values for code_location, used by fct TestProbe.\n",
    "\n",
    "code_speed_columns = ('Clock Time', 'Location', 'Exec Time')\n",
    "\n",
    "XTNS_COLS = ('stock_price_at_STO_offer', 'STO_ask_fee', 'STO_ask_fee_type',\n",
    "          'STO_offer_datetime', 'STO_strike', 'STO_expiry_datetime',\n",
    "          'STO_days_long', 'STO_offer_ARR', 'STO_offer_TQI', 'STO_offer_strike_delta',\n",
    "          'STO_xtn_state', 'STO_contracts', 'option_type', 'stock_price_BTC_offer',\n",
    "          'BTC_bid_fee', 'BTC_bid_fee_type', 'BTC_offer_datetime', 'BTC_contracts',\n",
    "          'stock_price_at_open', 'STO_open_fee', 'STO_open_commsn', 'STO_open_proceeds',\n",
    "          'STO_open_datetime', 'STO_open_ARR', 'STO_open_TQI', 'STO_open_strike_delta',\n",
    "          'stock_price_at_close', 'STO_close_strike_delta', 'BTC_close_commsn', 'BTC_close_proceeds',\n",
    "          'STO_close_datetime', 'STO_close_ARR', 'STO_close_TQI', 'Called_close_fee',\n",
    "          'Called_close_commsn', 'Called_close_proceeds', 'STO_xtn_net_proceeds'\n",
    "          )\n",
    "\n",
    "# Other xtn_cols to consider as features:  'STO_Offer_datetime', 'STO_open_strike_delta', 'STO_offer_strike_delta'     \n",
    "\n",
    "FEATURES_IN_XTNS_COLS = ('stock_price_at_STO_offer', 'STO_ask_fee', 'STO_strike', \\\n",
    "                         'STO_days_long', 'STO_offer_ARR', 'STO_offer_TQI', 'stock_price_at_open', 'Closed_xtn_Profit$', \\\n",
    "                         'Closed_xtn_Profit%','STO_open_strike_delta%', 'STO_offer_strike_delta%', 'STO_ask_fee%')\n",
    "\n",
    "MIN_XTN_FEATURES = ('stock_price_at_STO_offer', 'STO_strike', \\\n",
    "                    'STO_days_long', 'STO_offer_ARR', 'stock_price_at_open', 'Closed_xtn_Profit%', 'Closed_xtn_Profit$', \\\n",
    "                    'STO_open_strike_delta%', 'STO_offer_strike_delta%', 'STO_ask_fee%')  #10 features, plus the label (Closed Price Profit%. \n",
    "\n",
    "MIN_PRICE_FEATURES = ('45 day SMA', '30 day SMA', '15 day SMA', '10 day SMA', 'Yearly Min', 'Yearly Max', \\\n",
    "                      '5 days old', '4 days old', '3 days old', '2 days old', '1 days old', 'ER - Expiry') #12 features\n",
    "\n",
    "MIN_VELOCITY_FEATURES = ('45 day vel', '30 day vel', '15 day vel', '10 day vel', \\\n",
    "                     '5 day vel', '2 day vel')   # 6 features\n",
    "\n",
    "\n",
    "# VARIABLES\n",
    "\n",
    "xtns                    = pd.DataFrame()\n",
    "features                = pd.DataFrame()\n",
    "stock_price_features    = pd.DataFrame()\n",
    "stock_velocity_features = pd.DataFrame()\n",
    "stock_OHLC_data         = pd.DataFrame()\n",
    "corr_table              = pd.DataFrame()\n",
    "learner_stats           = pd.DataFrame()\n",
    "y_preds                 = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "- TestProbe - used for debugging DataGlue before moved to Notebook. May not need?\n",
    "- PriceFeatureCalc - inputs  = XOM OHLC.csv\n",
    "                   - outputs = SMA features, Yearly High/Low features, Days to ER\n",
    "- VelocityFeatureCalc - calculates price velocity features from stock price OHLC dataframe\n",
    "- RunLearners - runs a set of Learners with given train and test data.\n",
    "              - collects basic stats in a dataframe.\n",
    "- Chart_Features_2Plot - charts each feature with winners and losers in different colors. v1.0, not used.\n",
    "- Chart_Features_Label_Separation - v2.0 of the fct above. This one actually works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************\n",
    "def TestProbe(code_loc):\n",
    "#***********************\n",
    "   global test_config\n",
    "   global speed_row\n",
    "   global code_speed\n",
    "   global code_speed_row\n",
    "   global code_speed_columns\n",
    "\n",
    "   if 'golden test' in test_config:   # set up golden test configuration\n",
    "      # print('in TestProbe, code_loc = ', code_loc) #for debug\n",
    "      if code_loc == 'initialize':  #Set all Option strategy parameters to the kgolden test configuraiton\n",
    "         print(' *** in TestProbe *** golden test is in test_config.')\n",
    "\n",
    "         trade_days1   = pd.read_csv('DataGlue_GoldenTradeDays1.csv', index_col = False)\n",
    "         trade_days2   = pd.read_csv('DataGlue_GoldenTradeDays2.csv', index_col = False)\n",
    "         feature_days1 = pd.read_csv('DataGlue_GoldenFeatureDays1.csv', index_col = False)\n",
    "         feature_days2 = pd.read_csv('DataGlue_GoldenFeatureDays2.csv', index_col = False)\n",
    "\n",
    "\n",
    "   if 'code run times' in test_config: #Write the df to DataGlue_code_speed\n",
    "       print('here in TestProbe, module -code run times- we got nuttin.')\n",
    "       #probably no need for code_speed in this module\n",
    "       #code_speed.loc[speed_row, 'Clock Time'] = datetime.now()\n",
    "       #code_speed.loc[speed_row, 'Location']   = code_loc\n",
    "       #speed_row += 1\n",
    "      \n",
    "   if 'code locator' in test_config:\n",
    "      print('     ------')\n",
    "      print('     ', code_loc)\n",
    "      print('     ------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******\n",
    "def PriceFeatureCalc():\n",
    "    #\n",
    "    # Goal - Prepare stock price data features for gluing to the option xtn records\n",
    "    #        Check time of ER report. If after close, then ER day is OK.\n",
    "    #        If ER report before stock market open, then must use ER day - 1 for the join fct.\n",
    "    #        1. Download stock OHLC data from Yahoo or other. Remember to get early data for max calc offset (255 days for annual hi/low)\n",
    "    #        2. Choose data set to generate -\n",
    "    #           To start, try SMAs for 45, 30, 15, and 10 days\n",
    "    #           Also 12 month hi/lo. Need data for 12 months (255 days) prior to start day.\n",
    "    #           Also Closing price for last 5 days\n",
    "    #           days_to_ER\n",
    "    #        3. Calculate all elements of the data set using downloaded data file.\n",
    "\n",
    "    # Price Feature Config Parameters\n",
    "    global stock_OHLC_data\n",
    "    global stock_price_features\n",
    "\n",
    "  \n",
    "    print('+++ Here in PriceFeatureCalc')\n",
    "    SMA_periods = [45, 30, 15, 10]\n",
    "    days_per_year = 253\n",
    "    previous_days = 5\n",
    "    \n",
    "    stock_OHLC_data    = pd.DataFrame()\n",
    "    ER_Dates           = pd.DataFrame()\n",
    "\n",
    "    input_file_name    = 'XOMOHLC 2016-2018.csv'\n",
    "    output_file_name   = 'XOM_feature_set_2017.csv'\n",
    "    ER_input_file_name = 'XOM_ER_Dates.csv'\n",
    "\n",
    "    #+++++\n",
    "    #Read input files\n",
    "    stock_OHLC_data      = pd.read_csv(input_file_name, index_col = False)\n",
    "    ER_Dates             = pd.read_csv(ER_input_file_name, index_col = False)\n",
    "    stock_price_features = pd.read_csv(input_file_name, index_col = False)\n",
    "\n",
    "    #+++++\n",
    "    # Create price features - SMAs, hi/lows, and trailiing daily prices.\n",
    "    print('+++++ Calculating price features.')\n",
    "    \n",
    "    for days in SMA_periods:      #this loop so easy!\n",
    "       SMA_col_label = str(days) + ' day SMA'\n",
    "       stock_price_features[SMA_col_label] = stock_price_features['Close'].rolling(window = days).mean()\n",
    "\n",
    "    stock_price_features['Yearly Min'] = stock_price_features['Close'].rolling(days_per_year).min()\n",
    "    stock_price_features['Yearly Max'] = stock_price_features['Close'].rolling(days_per_year).max()\n",
    "\n",
    "    for prev_day in range(1,(previous_days+1)):\n",
    "       stock_price_features[str(prev_day) + ' days old'] = stock_price_features['Close'].shift(prev_day)\n",
    "\n",
    "    #+++++\n",
    "    # Create days to ER\n",
    "    # Convert time strings to datetime.date objects\n",
    "    print('+++++ Here in ER calc')\n",
    "    stock_price_features['Date'] = pd.to_datetime(stock_price_features['Date'], format = '%Y-%m-%d')\n",
    "\n",
    "#    ER_Dates['ER Date'] = pd.to_datetime(ER_Dates['ER Date'], format = '%Y-%m-%d')\n",
    "#copy start for Jupyter\n",
    "    stock_price_features['Days to ER']  = -999 #initialize this col with invalid value. If a -1 shows up, something broke.\n",
    "\n",
    "    for j in range(stock_price_features.shape[0]):\n",
    "        stock_price_features.loc[j, 'Days to ER'] = (pd.to_datetime(ER_Dates.loc[0, 'ER Date']) - \n",
    "                                                     pd.to_datetime(stock_price_features.loc[j,'Date'])).days\n",
    "        #print('days = ', stock_price_features.loc[j, 'Days to ER'])\n",
    "        if stock_price_features.loc[j, 'Days to ER'] <= 0:   #If passed this ER Date, then delete it and move to the next ER Date.\n",
    "                                                             # Note this loop cannot skip two ER Dates in a row. Must be at least one\n",
    "                                                             # features date between every ER Date. Pretty reasonable.\n",
    "                ER_Dates.drop(0, inplace = True)             # Note this destroys ER_Dates. DO NOT write it back to disk or use it again.\n",
    "                ER_Dates.reset_index(inplace = True, drop = True)\n",
    "                stock_price_features.loc[j, 'Days to ER'] = (pd.to_datetime(ER_Dates.loc[0, 'ER Date']) - \\\n",
    "                                                             pd.to_datetime(stock_price_features.loc[j,'Date'])).days\n",
    "\n",
    "    stock_price_features.to_csv('XOM_price_features.csv', index = False)\n",
    "    \n",
    "    #------\n",
    "    # Vector version of Days_to_ER. Get this working to use in notebook.\n",
    "    #-------\n",
    "##      stock_price_features['Days to ER'] = -999\n",
    "##      ER_Dates['ER Date']         = pd.to_datetime(ER_Dates['ER Date'])\n",
    "##      stock_price_features['Date'] = pd.to_datetime(stock_price_features['Date'])\n",
    "##                                           \n",
    "##      for ER_index in range(1, ER_Dates.shape[0]+1):\n",
    "##          print('type of last_ER_date = ', type(last_ER_Date))\n",
    "##          last_ER_Date = pd.to_datetime(ER_Dates.loc[ER_index-1, 'ER Date'])\n",
    "##          next_ER_Date = pd.to_datetime(ER_Dates.loc[ER_index,   'ER Date'])\n",
    "##\n",
    "##          print('Date col type = ', stock_price_features['Date'])\n",
    "##          print('Date element type = ', stock_price_features.loc[1,'Date'])\n",
    "##          stock_price_features['Date_outside_ER_range'] = (pd.to_datetime(stock_price_features['Date']) <=  last_ER_Date | \\\n",
    "##                                   pd.to_datetime(stock_price_features['Date']) > next_ER_Date)\n",
    "##          print('Date_outside_ER_range = ',type(stock_price_features['Date_outside_ER_range']))\n",
    "##                \n",
    "##          stock_price_features['Days to ER'] = stock_price_features['Days to ER'].where(stock_price_features['Date_outside_ER_range'], \\\n",
    "##                                               other = (next_ER_Date - pd.to_datetime(stock_price_features['Date'])).days, \\\n",
    "##                                               inplace = True)\n",
    "##          print()\n",
    "##          print('Vector solution')\n",
    "##          \n",
    "##          print(stock_price_features['Days to ER'].head(10))\n",
    "##          print(stock_price_features['Days to ER'].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           NaN\n",
      "1           NaN\n",
      "2           NaN\n",
      "3           NaN\n",
      "4           NaN\n",
      "5           NaN\n",
      "6           NaN\n",
      "7           NaN\n",
      "8           NaN\n",
      "9           NaN\n",
      "10          NaN\n",
      "11          NaN\n",
      "12          NaN\n",
      "13          NaN\n",
      "14    54.365552\n",
      "15    56.875767\n",
      "16    58.885336\n",
      "17    67.502652\n",
      "18    72.397739\n",
      "19    74.037420\n",
      "20    80.695483\n",
      "21    78.647292\n",
      "22    90.982419\n",
      "23    84.330644\n",
      "24    84.890279\n",
      "Name: RSI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#*******\n",
    "def OscillatorsFeatureCalc():\n",
    "    #\n",
    "    # Calculate several common oscillators\n",
    "    #     RSI - Relative Strength - Just a SMA of gains vs. losses over 14 periods.\n",
    "    #     MFI - volume weighted RSI\n",
    "    #     MACD - Convergence / Divergence of 16 and 26 day EMAs.\n",
    "    \n",
    "    global stock_OHLC_data\n",
    "    #global stock_price_features\n",
    "    RSI_period = 14\n",
    "\n",
    "    signals = pd.DataFrame()\n",
    "    \n",
    "    signals['Adj Close'] = stock_OHLC_data['Adj Close']\n",
    "    #print(signals['Adj Close'].head(25))\n",
    "    #print(signals['Adj Close'].diff().head(25))\n",
    "    signals['close delta'] = signals['Adj Close'].diff().dropna()\n",
    "    \n",
    "    signals['Upday'] = 0.0\n",
    "    signals['Upday'].where(signals['close delta']<=0.0,  other = signals['close delta'], inplace = True)\n",
    "    signals['Upsum'] = signals['Upday'].rolling(window = RSI_period).mean()\n",
    "\n",
    "    signals['Dnday'] = 0.0\n",
    "    signals['Dnday'].where(signals['close delta']>=0.0, other = -signals['close delta'], inplace = True)\n",
    "    signals['Dnsum'] = signals['Dnday'].rolling(window = RSI_period).mean()\n",
    "    \n",
    "    signals['RSI'] = 100 - (100 / (1 + (signals['Upsum'] / signals['Dnsum'])))\n",
    "    print(signals['RSI'].head(25))\n",
    "\n",
    "input_file_name    = 'XOMOHLC 2016-2018.csv'\n",
    "stock_OHLC_data    = pd.read_csv(input_file_name, index_col = False) \n",
    "    \n",
    "OscillatorsFeatureCalc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******\n",
    "def VelocityFeatureCalc():\n",
    "    #\n",
    "    # Calculate stock price velocities for various intervals.\n",
    "    # Use lineregress() to get the slope for each time interval.\n",
    "    \n",
    "    global stock_OHLC_data\n",
    "    global stock_velocity_features\n",
    "\n",
    "    stock_velocity_features['Date'] = stock_OHLC_data['Date']\n",
    "    v_periods = (45, 30, 15, 10, 5, 2)\n",
    "    max_row = stock_OHLC_data.shape[0]\n",
    "\n",
    "    stock_price_list = stock_OHLC_data['Close'].to_list()\n",
    "    max_row = len(stock_price_list)\n",
    "\n",
    "    for period in v_periods:\n",
    "             x_values = list(range(period))\n",
    "             #print('x_values = ', x_values)\n",
    "             velocity = []             #initialize velocity list. This will be added to stock_velocity df.\n",
    "             for i in range(period-1): # initialize first values as 'NaN'\n",
    "                    velocity.append('NaN')\n",
    "             for window_end in range(period - 1, max_row):\n",
    "                    y_values = stock_price_list[window_end-(period-1):window_end+1]               # get a slice of prices, width of window.\n",
    "                    slope, intercept, r_value, p_value, std_err = linregress(x_values, y_values)  # get the slope (velocity\n",
    "                    velocity.append(slope)\n",
    "             col_name = str(period) + ' day vel'\n",
    "             stock_velocity_features[col_name] = velocity\n",
    "\n",
    "    stock_velocity_features['Date'] = pd.to_datetime(stock_velocity_features['Date'], format = '%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******\n",
    "def RunLearners(Xr_train, Xr_test, yr_train, yr_test, all_run_stats, run_ID):\n",
    "   #\n",
    "   # Run several learners with same data sets.\n",
    "   # Record the performance stats in df learner_stats\n",
    "   # Can add variations of each learner - change depth for RandForest, neighbors for KNN, alpha for MLP etc.\n",
    "   #\n",
    "   # Create new df learner_this_run for each time fct runs. When complete, append stats_this_run to learner_stats\n",
    "\n",
    "   this_run_stats = pd.DataFrame()\n",
    "   y_pred_cols    = pd.DataFrame()\n",
    "   \n",
    "   learners = {'RandomForest_d7' : RandomForestClassifier(max_depth=7,  random_state=97),\n",
    "               'RandomForest_d9' : RandomForestClassifier(max_depth=9,  random_state=97),\n",
    "               'RandomForest_d11': RandomForestClassifier(max_depth=11, random_state=97),\n",
    "               'RandomForest_d13': RandomForestClassifier(max_depth=13, random_state=97),\n",
    "               'RandomForest_d15': RandomForestClassifier(max_depth=15, random_state=97),\n",
    "               'RandomForest_d17': RandomForestClassifier(max_depth=17, random_state=97),\n",
    "               'RandomForest_d19': RandomForestClassifier(max_depth=19, random_state=97),\n",
    "               'RandomForest_d?' : RandomForestClassifier(max_depth=None, random_state=97),\n",
    "               'KNNeighbors_2'   : KNeighborsClassifier(n_neighbors=2),\n",
    "               'KNNeighbors_3'   : KNeighborsClassifier(n_neighbors=3),\n",
    "               'KNNeighbors_4'   : KNeighborsClassifier(n_neighbors=4),\n",
    "               'MLPerceptron'    : MLPClassifier(alpha=1, max_iter=1000),\n",
    "               'SVC linear'      : svm.SVC(kernel = 'linear'),\n",
    "               'SVC poly'        : svm.SVC(kernel = 'poly'),\n",
    "               }\n",
    "\n",
    "   learner_types = learners.keys()\n",
    "   for learner in learner_types:\n",
    "      clf = learners[learner]\n",
    "      clf.fit(Xr_train, yr_train)\n",
    "\n",
    "      yr_pred = clf.predict(X_test)\n",
    "      y_pred_cols[learner] = yr_pred\n",
    "\n",
    "      print()\n",
    "      print(learner, ' accuracy      = ', metrics.accuracy_score(yr_test, yr_pred))\n",
    "      print(learner, ' avg. precison = ', metrics.average_precision_score(yr_test, yr_pred))\n",
    "\n",
    "      this_run_stats.loc[learner, 'Accuracy'] = metrics.accuracy_score(yr_test, yr_pred)\n",
    "      this_run_stats.loc[learner, 'Precision'] = metrics.average_precision_score(yr_test, yr_pred)\n",
    "\n",
    "   this_run_stats['run_ID'] = run_ID\n",
    "   all_run_stats = all_run_stats.append(this_run_stats)\n",
    "   print('all_run_stats = ')\n",
    "   print(all_run_stats)      \n",
    "   return all_run_stats, y_pred_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******\n",
    "def Chart_Features_2Plot(features, base_curve):\n",
    "   # This fct charts each features col. against base_curve.\n",
    "   # 1. Normalize all data in range 0-to-1, or -1-to-1\n",
    "   #      or just use left and right axes. Right axis for price, left axis for feature.\n",
    "   # 2. Plot 3 cols of charts, as many as needed.\n",
    "   # 3. Red is price, green is the feature.\n",
    "   print('show me some feature pix')\n",
    "   fig,axes =plt.subplots(10,3, figsize=(12, 15)) # 3 columns each containing 10 figures, total 30 features\n",
    "   winners = opt_sample[opt_sample['label']==1]    # define winners\n",
    "   losers  = opt_sample[opt_sample['label']==0]    # define losers\n",
    "   #print('winners = ')\n",
    "   #print(winners)\n",
    "\n",
    "   winners = winners.to_numpy()\n",
    "   losers  = losers.to_numpy()\n",
    "   opt_np = opt_sample.to_numpy()\n",
    "\n",
    "   ax=axes.ravel()# flat axes with numpy ravel\n",
    "\n",
    "   print('setting up bins.') \n",
    "   for i in range(len(opt_sample.columns.values)):\n",
    "     _,bins=np.histogram(opt_np[:,i],bins=40)\n",
    "     \n",
    "     ax[i].hist(losers[:,i],bins=bins,color='r',alpha=.5)# red color for malignant class\n",
    "     ax[i].hist(winners[:,i],bins=bins,color='g',alpha=0.3)# alpha is           for transparency in the overlapped region\n",
    "     print('assigned histogram, now try labels.')\n",
    "     ax[i].set_title(opt_sample.columns.values[i],fontsize=9)\n",
    "     ax[i].axes.get_xaxis().set_visible(False) # the x-axis co-ordinates are not so useful, as we just want to look how well separated the histograms are\n",
    "     ax[i].set_yticks(())\n",
    "   ax[0].legend(['losers','winners'],loc='best',fontsize=8)\n",
    "   plt.tight_layout()# let's make good plots\n",
    "   plt.show()\n",
    "\n",
    "def Chart_Feature_Label_Separation(opt_sample):\n",
    "   print('show me some pictures. opt_sample shape')\n",
    "   print(opt_sample.shape)\n",
    "   fig,axes =plt.subplots(10,3, figsize=(12, 9)) # 3 columns each containing 9 figures, total 27 features\n",
    "   winners = opt_sample[opt_sample['label']==1]    # define winners\n",
    "   losers  = opt_sample[opt_sample['label']==0]    # define losers\n",
    "   #print('winners = ')\n",
    "   #print(winners)\n",
    "\n",
    "   winners = winners.to_numpy()\n",
    "   losers  = losers.to_numpy()\n",
    "   opt_np = opt_sample.to_numpy()\n",
    "\n",
    "   ax=axes.ravel()# flat axes with numpy ravel\n",
    "\n",
    "   print('setting up bins.') \n",
    "   for i in range(len(opt_sample.columns.values)):\n",
    "     _,bins=np.histogram(opt_np[:,i],bins=40)\n",
    "     \n",
    "     ax[i].hist(losers[:,i],bins=bins,color='r',alpha=.5)# red color for malignant class\n",
    "     ax[i].hist(winners[:,i],bins=bins,color='g',alpha=0.3)# alpha is           for transparency in the overlapped region\n",
    "     print('assigned histogram, now try labels.')\n",
    "     ax[i].set_title(opt_sample.columns.values[i],fontsize=9)\n",
    "     ax[i].axes.get_xaxis().set_visible(False) # the x-axis co-ordinates are not so useful, as we just want to look how well separated the histograms are\n",
    "     ax[i].set_yticks(())\n",
    "   ax[0].legend(['losers','winners'],loc='best',fontsize=8)\n",
    "   plt.tight_layout()# let's make good plots\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "#\n",
    "# MAIN PROGRAM STARTS HERE\n",
    "#\n",
    "# *****************\n",
    "\n",
    "#******************\n",
    "# XTNS df - Get and prep for joining.\n",
    "# 1. Calculate any new features in xtns.\n",
    "# 2. Drop non-features from xtns, but keep the date cols for joining.\n",
    "# *****************\n",
    "\n",
    "xtns = pd.read_csv('closed_xtns_.csv', index_col = False)\n",
    "\n",
    "# Calculate all additional feature cols from xtns\n",
    "xtns['STO_ask_fee%']            = xtns['STO_ask_fee'] / xtns['stock_price_at_STO_offer']\n",
    "xtns['STO_offer_strike_delta%'] = (xtns['STO_strike'] - xtns['stock_price_at_STO_offer']) / xtns['stock_price_at_STO_offer']\n",
    "xtns['STO_open_strike_delta%']  = (xtns['STO_strike'] - xtns['stock_price_at_open']) / xtns['stock_price_at_open']\n",
    "\n",
    "#These two are the goodness criteria. Add ClosedBTC when that is included in a run.\n",
    "xtns['STO_open_proceeds'] = (xtns['STO_open_fee'] * xtns['STO_contracts'] * 100) - xtns['STO_open_commsn']  \n",
    "xtns['Closed_xtn_Profit$'] = xtns['STO_open_proceeds']  # Profit$ for ClosedExpired trade\n",
    "xtns['Closed_xtn_Profit$'].where(xtns['STO_xtn_state'] == 'ClosedExpired', other = (xtns['STO_open_proceeds'] - \\\n",
    "           xtns['Called_close_commsn']) - ((xtns['stock_price_at_close'] - xtns['STO_strike']) * xtns['STO_contracts'] * 100), inplace = True)\n",
    "\n",
    "xtns['Closed_xtn_Profit%'] = xtns['Closed_xtn_Profit$'] / xtns['STO_open_proceeds'] # % = 1.00 for expired trdes.\n",
    "\n",
    "xtns.to_csv('xtns_glue.csv')\n",
    "\n",
    "\n",
    "#consider a quick print check for remaining columns\n",
    "\n",
    "#******************\n",
    "# FEATURES Creation - Get features based on stock price, stock price velocity, and ER Date. Prep for joining.\n",
    "# 1. Calculate any new features and add to features df.\n",
    "# 2. Drop non-features (or features not used in this ML data set) from features, but keep the date cols for joining.\n",
    "# *****************\n",
    "\n",
    "if 'price calc' in feature_select:\n",
    "    PriceFeatureCalc()\n",
    "#else:\n",
    "#    price_features = pd.read_csv('XOM_price_features.csv', index_col = False)\n",
    "\n",
    "if 'velocity calc' in feature_select:\n",
    "    VelocityFeatureCalc()\n",
    "#else:\n",
    "#    velocity_features = pd.read_csv('XOM_velocity_features.csv', index_col = False)\n",
    "\n",
    "#******************\n",
    "#\n",
    "# Join the features dfs - Join on the Date column - unique and common to bother feature dfs. \n",
    "# \n",
    "# *****************\n",
    "\n",
    "features = pd.merge(stock_price_features, stock_velocity_features, how='left', on='Date', sort=True, copy=True)\n",
    "features.to_csv('features.csv')\n",
    "\n",
    "#******************\n",
    "# JOIN 'Em - Join xtns with features in left, outer join. \n",
    "# 1. Calculate Join_Date, using CBDay to get previous business day, adjusting for NYSE holidays.\n",
    "# 2. Remove extra rows from features, to prevent explosion in outer join.\n",
    "# 3. Drop non-features (or features not used in this ML data set) from features. Use features lists for both xtns and features.\n",
    "# 4. Do the outer join.\n",
    "# 4. Remove Join_Date column\n",
    "# *****************\n",
    "\n",
    "xtns['Offer_Date'] = pd.to_datetime(xtns['STO_offer_datetime']).dt.normalize() # Discard time values by using ()dt.date. Use dt.normalize to reset time to 00:00:00\n",
    "xtns['Join_Date'] = xtns['Offer_Date'] + CBDay(-1, weekmask = 'Mon Tue Wed Thu Fri', holidays = NYSE_HOLIDAYS)\n",
    "#print('type of features[Date] = ', type(features.loc[0, 'Date']))\n",
    "\n",
    "features['Join_Date']    = features['Date']\n",
    "features_Join_Date_list  = features['Join_Date'].tolist()\n",
    "\n",
    "xtn_Join_Date_list = xtns['Join_Date'].tolist()\n",
    "\n",
    "#Remove extra rows from features. These are dates when no trades opened. Actually, no need for this. Left outer join does it auto.\n",
    "set_xtn_Join_Date = set(xtn_Join_Date_list)\n",
    "join_no_match = [x for x in features_Join_Date_list if x not in set_xtn_Join_Date]\n",
    "for del_date in join_no_match:\n",
    "    del_index = features[features['Join_Date'] == del_date].index\n",
    "    features.drop(del_index, inplace = True)\n",
    "\n",
    "# Do the left outer join to merge xtns df with features df. \n",
    "all_columns = pd.merge(xtns, features, how='left', on='Join_Date', sort=True, copy=True)\n",
    "#print('Merged xtn data with feature data')\n",
    "\n",
    "#******************\n",
    "# Calculate features that need both xtn data and stock price data\n",
    "#    Currently only feature that needs both is 'ER - Expiry'. So this is included in the MIN_PRICE_FEATURES list\n",
    "# *****************\n",
    "print('calculating ER - Expiry')\n",
    "all_columns.to_csv('all_columns.csv')\n",
    "all_columns['ER - Expiry'] = all_columns['Days to ER'] - all_columns['STO_days_long']\n",
    "\n",
    "all_columns.to_csv('all_columns.csv', index = False)\n",
    "print('+++++ Did the cale. Wrote the file to  all columns')\n",
    "#print(list(all_columns.columns))\n",
    "\n",
    "# Create master list of cols to save for this ML run.\n",
    "feature_cols = []\n",
    "feature_cols.extend(MIN_XTN_FEATURES)\n",
    "feature_cols.extend(MIN_PRICE_FEATURES)\n",
    "feature_cols.extend(MIN_VELOCITY_FEATURES)\n",
    "\n",
    "\n",
    "#print('feature_cols = ')\n",
    "#print(feature_cols)\n",
    "\n",
    "set_feature_cols = set(feature_cols)\n",
    "drop_cols = [x for x in list(all_columns) if x not in set_feature_cols] #remove features not selected for this ML data sset. Keep dates for joining.\n",
    "training_features = all_columns.drop(columns = drop_cols)\n",
    "training_features.to_csv('training_features_.csv', index = False)\n",
    "print('+++++ Columns for reduced training_features with only feature columns')\n",
    "print('training_features shape = ', training_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************\n",
    "#\n",
    "# Labels - Define labels and add column 'label' \n",
    "# \n",
    "# *****************\n",
    "# Define label = 1 if Closed_Xtn_Profit% > good_value\n",
    "\n",
    "min_profit_pct = .99  #\n",
    "training_features['label'] = 0\n",
    "training_features['label'].where(training_features['Closed_xtn_Profit%'] < min_profit_pct, other = 1, inplace = True)\n",
    "training_features.to_csv('training_features_wlabel.csv')\n",
    "drop_columns = ['Closed_xtn_Profit%', 'Closed_xtn_Profit$']\n",
    "training_features.drop(drop_columns, axis = 'columns', inplace = True)\n",
    "\n",
    "#******************\n",
    "#\n",
    "# Sample Size - Define sample size. Define sample method and select samples, using pd.sample fct..  \n",
    "# \n",
    "# *****************\n",
    "\n",
    "opt_sample = pd.DataFrame()\n",
    "opt_test   = pd.DataFrame()\n",
    "\n",
    "sample_size = 8000 # or your favorite integer\n",
    "test_size    = 2300\n",
    "#print(training_features.columns.to_list)\n",
    "opt_sample = training_features.sample(n = sample_size, random_state = 97)\n",
    "opt_not_in_sample = training_features.loc[~training_features.index.isin(opt_sample.index)]  #find all rows not_in_sample to get a collection for the test set.\n",
    "\n",
    "print('shape of not_in_sample is ', opt_not_in_sample.shape)\n",
    "print('shape of sample is ', opt_sample.shape)\n",
    "\n",
    "opt_test = opt_not_in_sample.sample(n = test_size, random_state = 83)\n",
    "\n",
    "opt_sample.to_csv('opt_sample.csv')\n",
    "opt_test.to_csv('opt_test.csv')\n",
    "print('wrote the sample and test files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************\n",
    "#\n",
    "# Create Train / Test Data sets.\n",
    "# 1. Random selection of records using. Shape train/test data sets for SKLearn - Convert from dataframe to ndarrays with OneHotEncoder.\n",
    "# 2. Encode category data using OneHotEncoder (ohe)\n",
    "#\n",
    "# *****************\n",
    "X_train = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "X_test  = pd.DataFrame()\n",
    "y_test  = pd.DataFrame()\n",
    "\n",
    "X_train = opt_sample.copy()\n",
    "#print(X_train.head(10))\n",
    "y_train = X_train[['label']].to_numpy()\n",
    "y_train = y_train.ravel()\n",
    "#print(y_train.shape)\n",
    "X_train.drop(labels = 'label', axis = 'columns', inplace = True)\n",
    "#print(X_train.shape)\n",
    "\n",
    "X_test = opt_test\n",
    "y_test = X_test[['label']].to_numpy()\n",
    "X_test.drop(labels = 'label', axis = 'columns', inplace = True)\n",
    "\n",
    "print('Whew. All done building the data set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************\n",
    "#\n",
    "# Visualizers - Histograms. \n",
    "# Show histograms of features grouped by label. See any separation visually for each feature.\n",
    "#\n",
    "#********************\n",
    "\n",
    "if visualize:\n",
    "   Chart_Feature_Label_Separation(opt_sample)\n",
    "\n",
    "#********************\n",
    "#\n",
    "# Feature Selection\n",
    "# 1. Seaborn Heat Map\n",
    "# 2. SelectKBest\n",
    "# 3. RFE\n",
    "# 4. \n",
    "#\n",
    "#********************\n",
    "\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "best_features = SelectKBest(score_func=chi2, k=10)\n",
    "fit = best_features.fit(X_train,y_train)\n",
    "\n",
    "feature_scores = pd.DataFrame(fit.scores_)\n",
    "feature_cols   = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfeatrure_cols,featrure_scores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features\n",
    "\n",
    "#********************\n",
    "#\n",
    "# GredSearchCV.  Tune yer hyperparameters.\n",
    "#\n",
    "#********************\n",
    "\n",
    "# Find the highet hill to climb.\n",
    "\n",
    "#********************\n",
    "#\n",
    "# Run several Learners. \n",
    "# \n",
    "#********************\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test  = np.ravel(y_test)\n",
    "learner_stats = pd.DataFrame()\n",
    "                 \n",
    "run_ID = 'Initial'\n",
    "learner_stats, y_preds = RunLearners(X_train, X_test, y_train, y_test, learner_stats, run_ID)\n",
    "learner_stats.to_csv('learner_stats.csv')\n",
    "y_preds.to_csv('learner_preds.csv')\n",
    "\n",
    "predictions = opt_test.merge(y_preds, how = 'left', left_index = True, right_index = True)\n",
    "predictions.to_csv('learner_predswlabels.csv')\n",
    "#['KNNeighbors_3'] = y_preds['KNNeighbors_3']\n",
    "#predictions['RandomForest_d15'] = y_preds['RandomForest_d15']\n",
    "\n",
    "#********************\n",
    "#\n",
    "# Feature Reduction\n",
    "# 1. Filter method. Run df.corr then pick features with abs(corr to label) > 0.5. Manual by inspection.\n",
    "# 2. Wrapper method. Feed features to a learner sequentially until accuracy maxes.\n",
    "#                    Backward Elimination - Start with all features. Eliminate weakest feature until model perofrmance peaks.\n",
    "#                    Recursive Feature Elimination (RFE)\n",
    "# 3. Embedded Method\n",
    "# SelectKBest\n",
    "# Corer Matrix with Heatmap\n",
    "#********************\n",
    "\n",
    "X = data.loc[:,0:20]  #independent columns\n",
    "y = data.loc[:,-1]    #target column i.e price range\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features\n",
    "\n",
    "#********************\n",
    "#\n",
    "# PCA Analysis - PCA is Unsupervised Learning, so do not include the label in the data set. \n",
    "# Convert features df to ndarray - very tough. Use reshape and ravel.\n",
    "# Note Standard Scaler does feature normalization.\n",
    "#\n",
    "# To visualize relative contributions of each feature, use:\n",
    "#   1. heat map with seaborn.\n",
    "#   2. heat map with relative contributions to each PCA.\n",
    "#   3. overlapping histograms of each feature, grouped by label. Very Helpful.\n",
    "#\n",
    "#********************\n",
    "\n",
    "opt_sample_nolabel = opt_sample.drop('label', axis = 'columns')\n",
    "opt_sample_nda = opt_sample_nolabel.to_numpy()                    #create numpy data array with features only (drop labels)\n",
    "#print(opt_sample_nda)\n",
    "print(opt_sample_nda.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(opt_sample_nda)\n",
    "print('scaler.mean_ = ', scaler.mean_)\n",
    "opt_sample_xform = scaler.transform(opt_sample_nda)\n",
    "\n",
    "pca_n2 = PCA(n_components = 2)\n",
    "PrincipalComponents_options = pca_n2.fit_transform(opt_sample_xform)  #Try with with plain opt_sample_nda\n",
    "\n",
    "opt_PC_df = pd.DataFrame(data = PrincipalComponents_options, columns = ['principal component 1', 'principal component 2'])\n",
    "opt_PC_df.tail() # for visual inspection\n",
    "print('Explained variance per PC: {}'.format(pca_n2.explained_variance_)) \n",
    "\n",
    "print('pca components')\n",
    "print(pca_n2.components_)\n",
    "print()\n",
    "PCA_factors = pd.DataFrame()\n",
    "PCA_factors = pd.DataFrame(pca_n2.components_,columns=opt_sample_nolabel.columns,index = ['PC-1','PC-2'])\n",
    "print(PCA_factors)\n",
    "\n",
    "PCA_factors.to_csv('PCA_factors.csv')\n",
    "\n",
    "print('+++++')\n",
    "print('             Thats all, folks.')\n",
    "print('+++++')\n",
    "\n",
    "sys.exit('Bye, bye')\n",
    "      \n",
    "#********************\n",
    "#\n",
    "# Dimensionality Reduction - Idenrtify and remove features with low information.\n",
    "#\n",
    "#********************\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
