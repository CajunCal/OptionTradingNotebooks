{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Option Trading with ML\n",
    "## Feature Selection\n",
    "What could possibly go wrong?\n",
    "12/02/2020\n",
    "Develop in this notebook. Leave D1 as the working demo until replaced by another fully working demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this Notebook - Select the Best Features from the Feature Set\n",
    "In the first notebook in this series we created a set of features from stock price information and earnings report date. Do these features actually contain enough information for a Learner to predict profitable stock option trades? In this notebook we will evaluate the features using four techniques to determine the quality of each feature. Finally, you will choose the best features to use in training the Learners.\n",
    "\n",
    "Good features can help a Learner perform well. Conversely, bad features can cause many problems for a Learner, such as over-fitting and long train and test times. Leakage of future information into a feature set can cause completely erroneous results and failure of the entire project.\n",
    "\n",
    "## Contents\n",
    "As discussed above, feature reduction and elimination are powerful tools to improve Learner performance. Many techniques are available, so consider these four as just a sample to get you started.\n",
    "\n",
    "0. Scaling - Scaling is not really a feature reduction tool. Consider it more of a feature improvement tool. Learners prefer to work with features that have similar scale and distribution. This simplifies the mathematical analysis and reduces scaling errors. StandardScaler converts every feature to a common normal distribution - usually N(0,1). Feature scaling has many complexities, so read the docs to understand how it can affect your features, especially features with non-normal distributions.\n",
    "\n",
    "1. SelectKBest - univariate feature selection and ranking. SelectKBest scores each feature individually, then ranks the features according to the scores.\n",
    "\n",
    "2. Correlation with Heat Map - Using the pandas corr function and the Seaborn library, we can map the cross-correlations of each feature with every other feature and, most important, with the label.\n",
    "\n",
    "3. RFE - Recursive Feature Elimination. RFE is a powerful function in the SKLearn library. It recursively tests groups of features together, gradually eliminating the weakest ones, until it arrives at the smallest ensemble of features that produces the best results. Read the docs to learn more about RFE.\n",
    "\n",
    "4. Principal Component Analysis (PCA) - PCA is not really a feature reduction tool. It performs dimension reduction, by combining features into a few, typically two or three, artificial features by combining the original features mathematically to preserve the maximum independent variance. For problems with many dimensions, say hundreds or more, PCA can be very effective.\n",
    "\n",
    "5. Shapley Adaptive Model SHAP\n",
    "\n",
    "- What You Need: Basic knowledge of Python, pandas, numpy, dataframes, and statistics.\n",
    "\n",
    "- What You Don't Need: Knowledge of linear algebra, calculus, or Machine Learning. The Python libraries, particularly scipy and SKLearn for ML, do all the heavy lifting.\n",
    "\n",
    "You Will Learn:\n",
    "\n",
    "- How to scale a feature set with StandardScaler.\n",
    "- Three feature selection methods.\n",
    "- Principal Component Analysis (PCA)\n",
    "- a few more capabilities of MatPlotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler and SelectKBest\n",
    "\n",
    "Evaluation Questions\n",
    "- What is the importance of feature scaling?\n",
    "- Before you look at the results, choose the three features that you rank highest.\n",
    "- How do your choices compare with the ranking of SelectKBest? Any surprises? Why?\n",
    "- Given the scores from SelectKBest, how many features woyuld you choose for the training set? Why?\n",
    "\n",
    "Code Questions\n",
    "- What is the distribution of the features after running StandardScaler? Print a few to confirm?\n",
    "- What happens to the mean and standard deviation values of each original feature? Print a few.\n",
    "- How does SelectKBest calculate the feature score?\n",
    "- Create a dataframe to collect the scores and ranking for each feature selection algorithm. \n",
    "- Use this dataframe to compare the results at the end of this notebook.\n",
    "- The features ER - Expiry and 2 day Velocity show NaNs as scores. Why? Can you change these scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5577, 29)\n",
      "['stock_price_at_STO_offer', 'STO_strike', 'STO_days_long', 'STO_offer_ARR', 'stock_price_at_open', 'STO_ask_fee%', 'STO_offer_strike_delta%', '45 day SMA', '30 day SMA', '15 day SMA', '10 day SMA', 'Yearly Min', 'Yearly Max', '1 days old', '2 days old', '3 days old', '4 days old', '5 days old', 'RSI', 'MFI', 'MACD', '45 day vel', '30 day vel', '15 day vel', '10 day vel', '5 day vel', '2 day vel', 'ER - Expiry', 'label']\n",
      "    KBest Score                   Feature  KBest_rank\n",
      "0        1355.0                30 day SMA           0\n",
      "1        1231.0                45 day SMA           1\n",
      "2        1010.0                STO_strike           2\n",
      "3         850.0                15 day SMA           3\n",
      "4         750.0                10 day SMA           4\n",
      "5         744.0                3 days old           5\n",
      "6         738.0                4 days old           6\n",
      "7         696.0   STO_offer_strike_delta%           7\n",
      "8         670.0                5 days old           8\n",
      "9         665.0                2 days old           9\n",
      "10        605.0                1 days old          10\n",
      "11        581.0       stock_price_at_open          11\n",
      "12        563.0  stock_price_at_STO_offer          12\n",
      "13        521.0             STO_offer_ARR          13\n",
      "14        423.0              STO_ask_fee%          14\n",
      "15        420.0                45 day vel          15\n",
      "16        198.0                Yearly Min          16\n",
      "17        143.0             STO_days_long          17\n",
      "18        126.0                      MACD          18\n",
      "19         99.0               ER - Expiry          19\n",
      "20         81.0                       MFI          20\n",
      "21         80.0                Yearly Max          21\n",
      "22         15.0                 5 day vel          22\n",
      "23          9.0                30 day vel          23\n",
      "24          1.0                 2 day vel          24\n",
      "25          0.0                       RSI          25\n",
      "26          0.0                15 day vel          26\n",
      "27          0.0                10 day vel          27\n",
      "28          NaN                     label          28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing     import StandardScaler\n",
    "from sklearn.decomposition     import PCA\n",
    "\n",
    "raw_features = pd.read_csv('opt_sample.csv', index_col = 0)\n",
    "\n",
    "# Remove duplicate features\n",
    "dup_features = ['STO_open_strike_delta%', 'stock_price_at_open']\n",
    "print(raw_features.shape)\n",
    "raw_features_headers = raw_features.columns.to_list() \n",
    "print(raw_features_headers)\n",
    "features_np = raw_features.to_numpy(copy = True)\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features_np)\n",
    "\n",
    "X_train = features[:,:28]  #independent features (columns)\n",
    "y_train = features[:,-1]    #label column\n",
    "\n",
    "# Apply SelectKBest class to measure and rank features.\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k='all')\n",
    "fit = bestfeatures.fit(X_train,y_train)\n",
    "feature_scores = pd.DataFrame(fit.scores_)\n",
    "feature_columns = pd.DataFrame(raw_features.columns)\n",
    "\n",
    "feature_scores = pd.concat([feature_scores, feature_columns],axis=1)\n",
    "feature_scores.columns = ['KBest Score', 'Feature']\n",
    "feature_scores.sort_values(['KBest Score'], ascending = False, inplace = True)\n",
    "feature_scores = feature_scores.reset_index(drop = True)\n",
    "feature_scores['KBest Score'] = round(feature_scores['KBest Score']) \n",
    "feature_scores['KBest_rank'] = feature_scores.index\n",
    "print(feature_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-correlation and Seaborn Heat Map\n",
    "Evaluation questions\n",
    "- What row shows the cross-correlation of each feature to the labell?\n",
    "- Why is the correlation value exactly '1' for every cell on the diagonal?\n",
    "- What is the difference between +1 and -1 in correlation? Which has more information? Why? Why not?\n",
    "\n",
    "Code Questions\n",
    "- Run the correlation heat map with the unscaled features. What differences, and why?\n",
    "- Record the feature rankings in a column in your feature rank dataframe. Be sure to label it 'Cross-correlation.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  corr_rank     label\n",
      "0                 30 day SMA          0  0.442180\n",
      "1                 45 day SMA          1  0.425345\n",
      "2                 STO_strike          2  0.391565\n",
      "3                 15 day SMA          3  0.363756\n",
      "4                 10 day SMA          4  0.344375\n",
      "5                 3 days old          5  0.343056\n",
      "6                 4 days old          6  0.341922\n",
      "7    STO_offer_strike_delta%          7  0.333154\n",
      "8                 5 days old          8  0.327573\n",
      "9                 2 days old          9  0.326405\n",
      "10                1 days old         10  0.312986\n",
      "11       stock_price_at_open         11  0.307195\n",
      "12  stock_price_at_STO_offer         12  0.302931\n",
      "13                45 day vel         13  0.264699\n",
      "14                Yearly Min         14  0.185214\n",
      "15             STO_days_long         15  0.158391\n",
      "16                      MACD         16  0.148949\n",
      "17                30 day vel         17  0.039534\n",
      "18                       RSI         18  0.009135\n",
      "19                10 day vel         19  0.006621\n",
      "20                15 day vel         20 -0.007959\n",
      "21                 2 day vel         21 -0.011944\n",
      "22                 5 day vel         22 -0.051745\n",
      "23                Yearly Max         23 -0.119283\n",
      "24                       MFI         24 -0.119346\n",
      "25               ER - Expiry         25 -0.131845\n",
      "26              STO_ask_fee%         26 -0.265610\n",
      "27             STO_offer_ARR         27 -0.292417\n",
      "28                     label         28 -1.000000\n",
      "    KBest Score                   Feature  KBest_rank  \\\n",
      "0         605.0                1 days old          10   \n",
      "1         750.0                10 day SMA           4   \n",
      "2           0.0                10 day vel          27   \n",
      "3         850.0                15 day SMA           3   \n",
      "4           0.0                15 day vel          26   \n",
      "5           1.0                 2 day vel          24   \n",
      "6         665.0                2 days old           9   \n",
      "7         744.0                3 days old           5   \n",
      "8        1355.0                30 day SMA           0   \n",
      "9           9.0                30 day vel          23   \n",
      "10        738.0                4 days old           6   \n",
      "11       1231.0                45 day SMA           1   \n",
      "12        420.0                45 day vel          15   \n",
      "13         15.0                 5 day vel          22   \n",
      "14        670.0                5 days old           8   \n",
      "15         99.0               ER - Expiry          19   \n",
      "16        126.0                      MACD          18   \n",
      "17         81.0                       MFI          20   \n",
      "18          0.0                       RSI          25   \n",
      "19        423.0              STO_ask_fee%          14   \n",
      "20        143.0             STO_days_long          17   \n",
      "21        521.0             STO_offer_ARR          13   \n",
      "22        696.0   STO_offer_strike_delta%           7   \n",
      "23       1010.0                STO_strike           2   \n",
      "24         80.0                Yearly Max          21   \n",
      "25        198.0                Yearly Min          16   \n",
      "26          NaN                     label          28   \n",
      "27        563.0  stock_price_at_STO_offer          12   \n",
      "28        581.0       stock_price_at_open          11   \n",
      "\n",
      "                     feature  corr_rank     label  \n",
      "0                 1 days old         10  0.312986  \n",
      "1                 10 day SMA          4  0.344375  \n",
      "2                 10 day vel         19  0.006621  \n",
      "3                 15 day SMA          3  0.363756  \n",
      "4                 15 day vel         20 -0.007959  \n",
      "5                  2 day vel         21 -0.011944  \n",
      "6                 2 days old          9  0.326405  \n",
      "7                 3 days old          5  0.343056  \n",
      "8                 30 day SMA          0  0.442180  \n",
      "9                 30 day vel         17  0.039534  \n",
      "10                4 days old          6  0.341922  \n",
      "11                45 day SMA          1  0.425345  \n",
      "12                45 day vel         13  0.264699  \n",
      "13                 5 day vel         22 -0.051745  \n",
      "14                5 days old          8  0.327573  \n",
      "15               ER - Expiry         25 -0.131845  \n",
      "16                      MACD         16  0.148949  \n",
      "17                       MFI         24 -0.119346  \n",
      "18                       RSI         18  0.009135  \n",
      "19              STO_ask_fee%         26 -0.265610  \n",
      "20             STO_days_long         15  0.158391  \n",
      "21             STO_offer_ARR         27 -0.292417  \n",
      "22   STO_offer_strike_delta%          7  0.333154  \n",
      "23                STO_strike          2  0.391565  \n",
      "24                Yearly Max         23 -0.119283  \n",
      "25                Yearly Min         14  0.185214  \n",
      "26                     label         28 -1.000000  \n",
      "27  stock_price_at_STO_offer         12  0.302931  \n",
      "28       stock_price_at_open         11  0.307195  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1224x1224 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seaborn heat map\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(repr(feature_columns))\n",
    "feature_col_list = feature_columns[0].to_list()\n",
    "\n",
    "scaled_features = pd.DataFrame(features, columns = feature_col_list)\n",
    "plt.figure(figsize=(17,17))\n",
    "#plot heat map\n",
    "#g=sb.heatmap(scaled_features.corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "corr_matrix = scaled_features.corr(method = 'pearson')\n",
    "# Rename index col to 'feature name'. Then sort and reindex.\n",
    "# Set label corr to -1.0. This is a clumsy method to get the sort to work correctly.\n",
    "# Find better approach later (delete the label row?)\n",
    "corr_matrix.loc[corr_matrix['label'] == 1.0] = -1.0\n",
    "\n",
    "corr_matrix['feature'] = corr_matrix.index\n",
    "corr_matrix.sort_values(by = 'label', axis = 0, ascending = False, ignore_index = True, inplace = True)\n",
    "corr_matrix['corr_rank'] = corr_matrix.index\n",
    "print_cols = ['feature', 'corr_rank', 'label']\n",
    "print(corr_matrix[print_cols])\n",
    "# Sort corr_matrix and featrure_scores by feature name to get correct alignment before copying cols to feature_scores.\n",
    "# Can do this by concat also.\n",
    "corr_matrix.sort_values(by = 'feature', axis = 0, ascending = True, ignore_index = True, inplace = True)\n",
    "feature_scores.sort_values(by = 'Feature', axis = 0, ascending = True, ignore_index = True, inplace = True)\n",
    "\n",
    "feature_scores[print_cols] = corr_matrix[print_cols]\n",
    "print(feature_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE\n",
    "The Recursive Feature Eliminator (RFE) compares many sets of features to find the smallest set with the best results. Read the docs, then consider these questions.\n",
    "\n",
    "Evaluation Questions\n",
    "- How does RFE work?\n",
    "- What is the estimator. What does it do?\n",
    "\n",
    "Code Questions\n",
    "- Describe the arguments given to the RFE selector.\n",
    "- Time the execution of RFE selector. Why does it take so long to run?\n",
    "- Would this execution time typically be a problem in a real-world application? Why, or not?\n",
    "- Write a function to run RFE with a range of values. Any differences in the results? Why?\n",
    "- Record the results of RFE ranking for the features in your feature ranking dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6c28529f1e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mRFE_ET_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRF_ET_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richard\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richard\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\richard\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination - continuous model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#estimator = SVR(kernel=\"linear\")\n",
    "#selector = RFE(estimator, n_features_to_select = 1, step=1, verbose = 2)\n",
    "#print('starting selector.')\n",
    "#selector = selector.fit(X_train, y_train)\n",
    "#print('finished selector.')\n",
    "#print(selector.support_)   # selected features\n",
    "#print(selector.ranking_) # feature rank\n",
    "#RFE_rank = np.transpose(RFE_rank).copy()\n",
    "\n",
    "# SVR feature rank shown below.\n",
    "#RFE_rank = [9, 8, 27, 21, 24, 26, 22, 14, 10, 3, 1, 6, 7, 18, 20, 23, 12, 19, 15, 13, 2, 4, 5, 11, 16, 17, 28, 25]\n",
    "#RFE_rank.append(len(RFE_rank)+1)\n",
    "\n",
    "# RFE - Classifier version\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost          import XGBClassifier\n",
    "import catboost\n",
    "from catboost         import CatBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "RFE_ET_rank = clf.feature_importances_\n",
    "print(RF_ET_rank)\n",
    "\n",
    "stop\n",
    "\n",
    "#clf = RandomForestClassifier(random_state = 97, verbose = 1)\n",
    "#clf = clf.fit(X_train, y_train)\n",
    "#RFE_RF_rank = clf.feature_importances_\n",
    "#print(RFE_RF_rank)\n",
    "\n",
    "RFE_feature_df = pd.DataFrame(RFE_rank, columns = ['RFE_rank'])\n",
    "RFE_feature_df['RFE_feature'] = raw_features_headers\n",
    "print(RFE_feature_df)\n",
    "RFE_feature_df.sort_values(by = 'RFE_feature', axis = 0, ascending = True, ignore_index = True, inplace = True)\n",
    "RFE_copy_cols = ['RFE_rank', 'RFE_feature']\n",
    "feature_scores[RFE_copy_cols] = RFE_feature_df[RFE_copy_cols]\n",
    "print(feature_scores)\n",
    "feature_scores.to_csv('feature_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature ranking, add Permutation Importance and Shapley Addative (SHAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "# model = Ridge(alpha=1e-2).fit(X_train, y_train)\n",
    "# model.score(X_val, y_val)\n",
    "\n",
    "# r = permutation_importance(model, X_val, y_val,\n",
    "#                           n_repeats=30,\n",
    "#                           random_state=0)\n",
    "\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#        print(f\"{diabetes.feature_names[i]:<8}\"\n",
    "#              f\"{r.importances_mean[i]:.3f}\"\n",
    "#              f\" +/- {r.importances_std[i]:.3f}\")\n",
    "\n",
    "import shap\n",
    "#explainer = shap.TreeExplainer(model)\n",
    "#shap_values = explainer.shap_values(X_train)\n",
    "#shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Principal Component Analysis (PCA) extracts the non-correlated information in each given feature and systhesizes a specified number of orthogonal new ((artificial) components. Thus, it performs significant dimensionality reduction.\n",
    "\n",
    "Evalution Questions\n",
    "- How is PCA different from feature ranking?\n",
    "- What are some benefits of dimensionality reduction?\n",
    "- What are some drawbacks of PCA?\n",
    "- Describe explained variance (ev), and explained variance ratio (evr).\n",
    "- Woud you consider the ev and evr shown to be adequate? Why, or not?\n",
    "\n",
    "Code Questions\n",
    "- Run PCA with different arguments. What arguments produce better results?\n",
    "- Run PCA with non-scaled features. Any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition     import PCA\n",
    "\n",
    "X_reduced = PCA(n_components = 2).fit(scaled_features)\n",
    "print(X_reduced.components_)\n",
    "print(X_reduced.explained_variance_)\n",
    "print(X_reduced.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Like PCA, LDA synthesizes artificial features (components) based on the non-correlated information contained in each of the given featuures. However, unlike PCA, LDA considers the labels, so it maximizes the separation of data clusters along label boundaries. PCA, which does not consider labels, simply identifies clusters and maximizes the separation of the clusters themselves.\n",
    "\n",
    "Evaluation Questions\n",
    "- Is LDA supervised or unsupervised?  PCA? \n",
    "- Which one do you expect to perform better for this problem? Why, or not?\n",
    "- How will you measure and evaluate their comparative performance?\n",
    "\n",
    "Code Questions\n",
    "- Create scatterplots of PCA and LDA. What insight do these provide?\n",
    "- Use the Learner function in Notebook 3 to run a set of Learners with PCA and LDA features. \n",
    "- What differences in Learning performance and train/test execution times? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 2)\n",
    "transformed = lda.fit_transform(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Your Features\n",
    "Now you have all the information you need to select the features you will use to train the Learners. \n",
    "\n",
    "Evaluation Questions\n",
    "- Did any of the rankings surprise you? Which ones? Why?\n",
    "- Each feature selector generated a different ranking for the features. Why?\n",
    "- Describe comparative strengths and weaknesses of each feature selector. \n",
    "- Which feature selector do you prefer? Why?\n",
    "- Would you combine the rankings to get a better overall feature rank? How?  Why, or not?\n",
    "- What is your ranking for the features? Why?\n",
    "\n",
    "Code Questions\n",
    "- Compare the rankings of each feature selector by graphing the scores with MatplotLib. \n",
    "- Write a function to do grouped bar charts, starting with the function in the next notebook that displays grouped bar charts. - How can you display the information in the most effective way to showcase your selection?\n",
    "\n",
    "\n",
    "I am eager to hear of your adventures in learning ML and especially in your challenges and results in tackling your own applications for ML. Please email me with questions, comments, and results of your work. \n",
    "\n",
    "I love to hear stories about experiences in ML, so give me a shout.\n",
    "\n",
    "Best wishes for selecting powerful features!\n",
    "\n",
    "Richard Barrett\n",
    "\n",
    "richard at rbshomes dot com\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
